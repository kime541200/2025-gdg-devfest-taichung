x-common-healthcheck: &llamacpp-common-healthcheck
  test: "curl -f http://localhost:8080/health || exit 1"
  interval: 15s
  timeout: 10s
  retries: 3
  start_period: 120s

services:
  gemma-3-27b-it:
    labels:
      - context_len=16k
      - max_context_len=128k
      - official_recommend="temperature = 1.0, top_k = 64, top_p = 0.95, min_p = 0.0, repeat-penalty = 1.0"
    init: true
    image: ghcr.io/ggml-org/llama.cpp:server-cuda
    container_name: gemma-3-27b-it
    environment:
      - TRANSFORMERS_OFFLINE=1    
    ports:
      - "15412:8080"
    volumes:
      - type: bind
        source: ./model
        target: /model
    env_file:
      - .env
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: ["gpu"]
              device_ids: ["0"]
    restart: always
    entrypoint: ""
    healthcheck: *llamacpp-common-healthcheck
    command: >
      ./llama-server
      -m /model/${MODEL_FILENAME}.gguf
      -a ${MODEL_FILENAME}
      --host 0.0.0.0
      --port 8080
      --no-webui
      -fa auto
      --jinja
      --temp 1.0
      --top_p 0.95
      --top_k 64
      --min_p 0
      --repeat-penalty 1.0
      -np 1
      -ngl 99
      -c 16384